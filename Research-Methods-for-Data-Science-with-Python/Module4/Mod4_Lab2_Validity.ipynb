{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 4, Lab 2 - Validity\n==========================\n\nIn this lab, we build on the previous lab to validate our measure of\nconsumer sentiment. In the previous lab, we found that we could make a\nreliable, one-factor measure that we think represents consumer\nsentiment. However, we did not show that the measure was actually\nmeasuring sentiment. In this lab, we discuss the concept of measurement\nvalidity--ensuring that the measure actually captures what it claims to.\n\nThere is no sure-fire way to ensure that a measure is valid. However,\nthere are some things we can consider.\n\n1.  **Face validity:** does the measure look valid, at face level? This is\n    subjective, but it is important. For example, in the previous lab,\n    we captured people's feelings about a taco brand with reported\n    rating on four adjectives: \"inviting,\" \"friendly,\" \"awesome,\" and\n    \"pleasant.\" We asked people to rate how well each adjective\n    describes the brand on a 1-10 scale. We can think through whether\n    these seem like they would be capturing sentiment. Clearly, a person\n    who rates a brand highly on \"awesome\" feels positive toward the\n    brand, right? Well, we can think through this a bit more. What other\n    reasons might a person respond positively on that question? Might\n    they just be an enthusiastic person? Perhaps someone who wants to\n    make a good impression or feels social pressure from the survey to\n    give a positive response? Next, we might consider whether those\n    issues would be shared among all the adjectives. Indeed, it seems\n    possible. There are still some other issues we might consider. For\n    example, do people really know and have the ability to report their\n    attitudes and feelings toward a brand accurately? Might something\n    else such as an analysis of their natural language be better? In\n    short, this measure *looks* face valid, but can also conceive of\n    some potential threats to its validity. We then would have to ask\n    whether those issues are big enough to warrant not using it.\n\n2.  **Content validity:** does the measure have the appropriate breadth?\n    With this set of adjectives, we might consider other words that\n    should be included, or we might consider removing some that don't\n    belong. This is also subjective. Indeed, in the last lab we were\n    considering discarding \"quirky\" before the data analysis came in.\n\n3.  **Criterion validity:** does the variable correlate in ways that a good\n    measure should? This is the \"data-driven\" option, but it is not\n    without issues as well. For example, it presumes that you have\n    picked a good set of outcomes to correlate it with and that those\n    are also measured validly. Still, assuming you are collecting data,\n    it doesn't hurt to collect a little bit more or to check this in the\n    data that you have collected.\n\nIn this lab, I briefly demonstrate **criteiron validity** by checking\ncorrelations among measures.\n\nLoad Data\n=========\n\nIn this lab, I use a slightly different measure of sentiment but a\nsimilar research design. A research team has a new sentiment measure,\nand they wish to know if it is criterion valid. To validate it, they\nhave assembled data on the number of positive words used to describe a\nproduct (word count, or `WC`), the rating of a product using a standard\nrating system (`rating`), and the anticipated likelihood of purchase\n(`purchase`). The data are in the github folder for this lab:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#### LOAD DATA ####\nimport pandas as pd\ndat = pd.read_csv(\"datasets/validity.csv\", index_col = 'Unnamed: 0')",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can briefly check the data. Notice there is an id variable called `X`\nas well."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(dat.dtypes)\n\ndat.head()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "sent        int64\nWC          int64\nrating      int64\npurchase    int64\ndtype: object\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>WC</th>\n      <th>rating</th>\n      <th>purchase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>1</td>\n      <td>6</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>2</td>\n      <td>7</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>4</td>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9</td>\n      <td>2</td>\n      <td>7</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   sent  WC  rating  purchase\n1     5   3       5         2\n2    10   1       6         4\n3     8   2       7         3\n4     6   4       6         3\n5     9   2       7         4"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Is the sentiment measure criterion valid? We can test this by simply\nassessing correlations."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#correlations\ncorr_mat = dat.corr().round(2)\ncorr_mat",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>WC</th>\n      <th>rating</th>\n      <th>purchase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sent</th>\n      <td>1.00</td>\n      <td>0.30</td>\n      <td>0.69</td>\n      <td>0.55</td>\n    </tr>\n    <tr>\n      <th>WC</th>\n      <td>0.30</td>\n      <td>1.00</td>\n      <td>0.15</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>rating</th>\n      <td>0.69</td>\n      <td>0.15</td>\n      <td>1.00</td>\n      <td>0.32</td>\n    </tr>\n    <tr>\n      <th>purchase</th>\n      <td>0.55</td>\n      <td>0.05</td>\n      <td>0.32</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "          sent    WC  rating  purchase\nsent      1.00  0.30    0.69      0.55\nWC        0.30  1.00    0.15      0.05\nrating    0.69  0.15    1.00      0.32\npurchase  0.55  0.05    0.32      1.00"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that the sentiment variable correlates at .30 with word\ncount, .69 with a product rating, and .55 with purchase likelihood. We\nalso see that those correlations are larger than they are between the\nother measures (e.g., the `rating` variable is not correlating more\nstrongly with the other variables).\n\nWe can easily compute the confidence intervals of these correlation coefficients. However, this requires a few steps which we first encountered in the Association lab:   \n1. Transform the correlation from the initial space which we call r to a transformed space z. The distribution of errors is Normal in this transformed space. \n2. Compute the CI in the transformed space.\n3. Transform back to the original space."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport scipy.stats as ss\nimport math\n\ndef r_z(r):\n    return math.log((1 + r) / (1 - r)) / 2.0\n\ndef z_r(z):\n    e = math.exp(2 * z)\n    return((e - 1) / (e + 1))\n\ndef r_conf_int(r, alpha, n):\n    # Transform r to z space\n    z = r_z(r)\n    # Compute standard error and critcal value in z\n    se = 1.0 / math.sqrt(n - 3)\n    z_crit = ss.norm.ppf(1 - alpha/2)\n\n    ## Compute CIs with transform to r\n    lo = z_r(z - z_crit * se)\n    hi = z_r(z + z_crit * se)\n    return (lo, hi)\n\ndef print_cis(corr_mat,var1, var2, idx1, idx2):\n    print('\\nFor ' + var1 + ' vs. ' + var2)\n    conf_ints = r_conf_int(corr_mat[idx1,idx2], 0.05, 1000)\n    print('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[idx1,idx2], conf_ints[0], conf_ints[1]))\n\ncorr_mat = np.array(corr_mat)\n\nprint_cis(corr_mat, 'sent', 'WC', 1, 0)\nprint_cis(corr_mat, 'sent', 'rating', 0, 2)\nprint_cis(corr_mat, 'sent', 'purchase', 0, 3)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\nFor sent vs. WC\nCorrelation = 0.300 with CI of 0.243 to 0.355\n\nFor sent vs. rating\nCorrelation = 0.690 with CI of 0.656 to 0.721\n\nFor sent vs. purchase\nCorrelation = 0.550 with CI of 0.505 to 0.592\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see that the confidence intervals for correlations with sentiment are all small. It appears that all three correlations are significant.\n\nAlthough this is a short exercise, it is critically important. Not only\ncan you quickly see if a measure is valid, but you might also find a\nbetter measure. For example, if the product rating was correlating with\npurchase likelihood *better* than the sentiment measure, I would\nquestion the use of the sentiment measure for making decisions."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}